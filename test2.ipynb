{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exoplanet Hunter: An AI/ML Approach to Automated Discovery\n",
    "\n",
    "This Jupyter Notebook demonstrates a simplified workflow for building an AI/ML model to detect exoplanets using the transit method. We'll generate synthetic light curves, preprocess them, extract features, train a machine learning model, and evaluate its performance.\n",
    "\n",
    "**Goal:** Develop an AI/ML model capable of automatically identifying exoplanets from stellar light curve data.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's ensure we have all the necessary Python libraries installed. If you're running this in a new environment, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightkurve scikit-learn numpy matplotlib pandas scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the required modules for data handling, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightkurve import LightCurve # A powerful library for Kepler/TESS data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Simulation (Synthetic Light Curves)\n",
    "\n",
    "In a real-world scenario, you would download actual light curves from missions like Kepler or TESS using `lightkurve`. For this demonstration, we'll generate synthetic light curves to quickly create a dataset with both exoplanet transits and non-transit stellar variations.\n",
    "\n",
    "We will generate:\n",
    "-   **Exoplanet Light Curves:** Stars with a periodic dip in brightness, simulating a planet transit.\n",
    "-   **Non-Exoplanet Light Curves:** Stars with only stellar variability and noise.\n",
    "\n",
    "*(Note: The transit shape here is a simple box function for clarity, real transits have a more complex shape.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_light_curve(time, is_exoplanet=False, period=None):\n",
    "    \"\"\"\n",
    "    Generates a synthetic light curve with optional transit.\n",
    "    \"\"\"\n",
    "    # Baseline stellar flux with some variability/noise\n",
    "    flux = np.random.normal(loc=1.0, scale=0.001, size=len(time)) # Gaussian noise\n",
    "    flux += 0.005 * np.sin(time / 50) # Long-term stellar variability\n",
    "    flux += 0.002 * np.cos(time / 10) # Shorter-term variability\n",
    "\n",
    "    if is_exoplanet:\n",
    "        if period is None:\n",
    "            period = np.random.uniform(10, 100) # Random period for transit between 10 and 100 days\n",
    "        transit_duration = period * np.random.uniform(0.02, 0.08) # 2% to 8% of period\n",
    "        transit_depth = np.random.uniform(0.005, 0.02) # 0.5% to 2% depth\n",
    "\n",
    "        # Simulate transits within the observation window\n",
    "        for t0_offset in np.arange(time[0] - period, time[-1] + period, period):\n",
    "            # Create a simple box-shaped transit\n",
    "            in_transit = (time >= t0_offset - transit_duration / 2) & \\\n",
    "                         (time <= t0_offset + transit_duration / 2)\n",
    "            flux[in_transit] -= transit_depth\n",
    "\n",
    "    return LightCurve(time=time, flux=flux)\n",
    "\n",
    "# Define observation parameters\n",
    "time_base = np.arange(0, 90, 0.02) # 90 days of observations, 30-min cadence\n",
    "\n",
    "num_exoplanets = 100\n",
    "num_non_exoplanets = 400\n",
    "\n",
    "light_curves = []\n",
    "labels = [] # 1 for exoplanet, 0 for non-exoplanet\n",
    "periods = [] # Store periods used for synthetic exoplanets\n",
    "\n",
    "# Generate exoplanet light curves\n",
    "print(\"Generating synthetic exoplanet light curves...\")\n",
    "for _ in range(num_exoplanets):\n",
    "    p = np.random.uniform(10, 100)\n",
    "    lc = generate_synthetic_light_curve(time_base, is_exoplanet=True, period=p)\n",
    "    light_curves.append(lc)\n",
    "    labels.append(1)\n",
    "    periods.append(p)\n",
    "\n",
    "# Generate non-exoplanet light curves\n",
    "print(\"Generating synthetic non-exoplanet light curves...\")\n",
    "for _ in range(num_non_exoplanets):\n",
    "    lc = generate_synthetic_light_curve(time_base, is_exoplanet=False)\n",
    "    light_curves.append(lc)\n",
    "    labels.append(0)\n",
    "    periods.append(None) # No specific period for non-exoplanets\n",
    "\n",
    "print(f\"Generated {len(light_curves)} light curves (Exoplanets: {sum(labels)}, Non-Exoplanets: {len(labels) - sum(labels)}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Synthetic Light Curves\n",
    "\n",
    "Let's take a look at a couple of our synthetic light curves to understand what the model will be seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Find an example exoplanet LC\n",
    "exoplanet_idx = np.where(np.array(labels) == 1)[0][0]\n",
    "lc_exoplanet = light_curves[exoplanet_idx]\n",
    "period_exoplanet = periods[exoplanet_idx]\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lc_exoplanet.time.value, lc_exoplanet.flux.value, '.', markersize=2, alpha=0.7)\n",
    "plt.title(f\"Example Exoplanet Light Curve (Synthetic) - Period: {period_exoplanet:.2f} days\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Flux\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Find an example non-exoplanet LC\n",
    "non_exoplanet_idx = np.where(np.array(labels) == 0)[0][0]\n",
    "lc_non_exoplanet = light_curves[non_exoplanet_idx]\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(lc_non_exoplanet.time.value, lc_non_exoplanet.flux.value, '.', markersize=2, alpha=0.7)\n",
    "plt.title(\"Example Non-Exoplanet Light Curve (Synthetic)\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Flux\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing and Feature Engineering\n",
    "\n",
    "This is a crucial step for traditional machine learning models. We need to transform the raw light curve data into a set of numerical features that the model can learn from. For exoplanet transit detection, this typically involves:\n",
    "\n",
    "-   **Normalization:** Scaling flux values.\n",
    "-   **Trend Removal:** Removing long-term stellar variability that can obscure transits.\n",
    "-   **Period Folding:** Aligning the light curve based on a candidate orbital period to stack multiple transits, enhancing the signal.\n",
    "-   **Statistical Features:** Calculating metrics like standard deviation, skewness, minimum, etc.\n",
    "\n",
    "For this example, we will use the *known* synthetic period for exoplanets when folding. In a real project, a **periodogram analysis** (e.g., `lightkurve.periodogram.LombScarglePeriodogram`) would be used to estimate the most likely period for each light curve *before* folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_feature_engineer(lc, period=None, folded_bins=100):\n",
    "    \"\"\"\n",
    "    Preprocesses a light curve and extracts features.\n",
    "    Args:\n",
    "        lc (lightkurve.LightCurve): The input light curve.\n",
    "        period (float, optional): The period to fold the light curve on. If None, no folding.\n",
    "        folded_bins (int): Number of bins for the folded light curve features.\n",
    "    Returns:\n",
    "        pd.Series: A series of extracted features.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # 1. Remove NaN values and flatten (trend removal)\n",
    "    lc_clean = lc.remove_nans()\n",
    "    \n",
    "    if len(lc_clean) < 3:\n",
    "        # Not enough data for meaningful processing, return default features\n",
    "        default_features = {\n",
    "            'std_dev_flux': 0.0, 'median_flux': 1.0, 'min_flux': 1.0, 'max_flux': 1.0,\n",
    "            'percentile_10': 1.0, 'percentile_90': 1.0, 'skewness_flux': 0.0, 'kurtosis_flux': 0.0\n",
    "        }\n",
    "        for i in range(folded_bins):\n",
    "            default_features[f'folded_bin_{i}'] = 0.0\n",
    "        return pd.Series(default_features)\n",
    "\n",
    "    # Normalize before flattening for better trend removal\n",
    "    normalized_flux = lc_clean.flux / np.nanmedian(lc_clean.flux)\n",
    "\n",
    "    # Use lightkurve's flatten for robust trend removal\n",
    "    # Using a window_length appropriate for the data length\n",
    "    window_len = min(len(normalized_flux) // 10 * 2 + 1, 1001) # Ensure odd and not too large\n",
    "    if window_len < 3: window_len = 3\n",
    "    \n",
    "    try:\n",
    "        flattened_lc = LightCurve(time=lc_clean.time, flux=normalized_flux).flatten(window_length=window_len)\n",
    "        processed_flux = flattened_lc.flux.value # Get the numpy array\n",
    "    except Exception as e:\n",
    "        # Fallback to simple savgol_filter if flatten fails (e.g., too few data points)\n",
    "        print(f\"Warning: lightkurve.flatten failed ({e}). Falling back to savgol_filter.\")\n",
    "        window_len_savgol = min(len(normalized_flux) - 1 if len(normalized_flux) % 2 == 0 else len(normalized_flux), 51)\n",
    "        if window_len_savgol < 3: window_len_savgol = 3\n",
    "        processed_flux = normalized_flux - savgol_filter(normalized_flux, window_length=window_len_savgol, polyorder=3) + 1\n",
    "    \n",
    "    # 2. Feature Extraction (simple statistics)\n",
    "    features['std_dev_flux'] = np.std(processed_flux)\n",
    "    features['median_flux'] = np.median(processed_flux)\n",
    "    features['min_flux'] = np.min(processed_flux)\n",
    "    features['max_flux'] = np.max(processed_flux)\n",
    "    features['percentile_10'] = np.percentile(processed_flux, 10)\n",
    "    features['percentile_90'] = np.percentile(processed_flux, 90)\n",
    "    features['skewness_flux'] = pd.Series(processed_flux).skew()\n",
    "    features['kurtosis_flux'] = pd.Series(processed_flux).kurtosis()\n",
    "\n",
    "    # 3. Period Folding Features\n",
    "    folded_lc_features = np.zeros(folded_bins) # Initialize with zeros\n",
    "    if period is not None and not np.isnan(period):\n",
    "        try:\n",
    "            # Create a new LC object for folding the processed flux\n",
    "            temp_lc_for_folding = LightCurve(time=lc_clean.time, flux=processed_flux)\n",
    "            folded_lc = temp_lc_for_folding.fold(period=period, wrap_around=0.5)\n",
    "\n",
    "            # Bin the folded light curve to a fixed size for consistent features\n",
    "            # Use `bin` method of LightCurve for more robust binning\n",
    "            binned_folded_lc = folded_lc.bin(bins=folded_bins, method='median')\n",
    "            # Ensure the binned array has the correct size, filling with 1.0 if not enough data\n",
    "            folded_lc_features[:len(binned_folded_lc.flux.value)] = binned_folded_lc.flux.value\n",
    "            folded_lc_features[len(binned_folded_lc.flux.value):] = 1.0 # Fill remaining with baseline\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Could not fold light curve with period {period:.2f}: {e}\")\n",
    "            pass # Keep folded_lc_features as zeros/baseline if folding fails\n",
    "    else:\n",
    "        # If no period, or period is NaN, fill with baseline (e.g., 1.0 for normalized flux)\n",
    "        folded_lc_features.fill(1.0)\n",
    "\n",
    "    # Append folded curve features\n",
    "    for i, val in enumerate(folded_lc_features):\n",
    "        features[f'folded_bin_{i}'] = val\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "\n",
    "all_features = []\n",
    "print(\"Preprocessing and extracting features for all light curves...\")\n",
    "for i, lc in enumerate(light_curves):\n",
    "    # For synthetic data, we use the generated period if it's an exoplanet.\n",
    "    # For non-exoplanets, we can try a generic period or skip folding for simplicity\n",
    "    # A more advanced approach would involve a period search for every candidate.\n",
    "    period_to_use = periods[i] if labels[i] == 1 else np.random.uniform(20, 80) # Generic period for non-exoplanets\n",
    "    \n",
    "    features = preprocess_and_feature_engineer(lc, period=period_to_use)\n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "# Fill any remaining potential NaNs that might arise from edge cases in feature extraction\n",
    "features_df = features_df.fillna(0) \n",
    "print(\"Feature extraction complete.\")\n",
    "print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "\n",
    "# Display first few rows of the features DataFrame\n",
    "print(\"\\nSample of extracted features:\")\n",
    "display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Folded Light Curve\n",
    "\n",
    "Let's see what a folded light curve looks like. This is the 'signature' that the model learns to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "lc_exoplanet_processed = light_curves[exoplanet_idx].remove_nans()\n",
    "\n",
    "# Normalize and flatten first\n",
    "normalized_flux_exoplanet = lc_exoplanet_processed.flux / np.nanmedian(lc_exoplanet_processed.flux)\n",
    "window_len = min(len(normalized_flux_exoplanet) // 10 * 2 + 1, 1001) # Ensure odd and not too large\n",
    "if window_len < 3: window_len = 3\n",
    "flattened_lc_exoplanet = LightCurve(time=lc_exoplanet_processed.time, flux=normalized_flux_exoplanet).flatten(window_length=window_len)\n",
    "\n",
    "# Now fold it\n",
    "folded_lc = flattened_lc_exoplanet.fold(period=period_exoplanet, wrap_around=0.5)\n",
    "\n",
    "plt.plot(folded_lc.time.value, folded_lc.flux.value, '.k', markersize=4, alpha=0.5)\n",
    "plt.title(f\"Folded Light Curve of an Exoplanet (Period: {period_exoplanet:.2f} days)\")\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Normalized Flux\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0.97, 1.01) # Adjust ylim to better see transit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training (Random Forest Classifier)\n",
    "\n",
    "We'll use a Random Forest Classifier, which is an ensemble learning method robust to noisy data and good for classification tasks. It will learn to distinguish between exoplanet candidates and non-candidates based on the features we engineered.\n",
    "\n",
    "Since exoplanets are rare, we'll use `class_weight='balanced'` to help the model handle the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_df\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# stratify=y ensures that both training and test sets have similar proportions of exoplanets and non-exoplanets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Test set size: {len(X_test)} samples\")\n",
    "print(f\"Exoplanets in training set: {sum(y_train)}\")\n",
    "print(f\"Exoplanets in test set: {sum(y_test)}\")\n",
    "\n",
    "print(\"\\nTraining Random Forest Classifier...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1) # n_jobs=-1 uses all available cores\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "We'll evaluate our model using several metrics important for imbalanced classification:\n",
    "\n",
    "-   **Accuracy:** Overall correctness.\n",
    "-   **Precision:** Of all predicted exoplanets, how many are actually exoplanets? (Minimizes false positives).\n",
    "-   **Recall:** Of all actual exoplanets, how many did we correctly identify? (Minimizes false negatives).\n",
    "-   **F1-score:** Harmonic mean of precision and recall.\n",
    "-   **ROC AUC:** Area Under the Receiver Operating Characteristic Curve, a robust metric for binary classifiers, less sensitive to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1] # Probability of being an exoplanet\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Non-Exoplanet', 'Predicted Exoplanet'],\n",
    "            yticklabels=['Actual Non-Exoplanet', 'Actual Exoplanet'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_score(y_test, y_prob):.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "For Random Forest, we can also inspect which features were most important in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "forest_importances = forest_importances.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=forest_importances.head(20).values, y=forest_importances.head(20).index, palette='viridis')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.xlabel(\"Mean Decrease in Impurity\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"As expected, the 'folded_bin' features (which capture the shape of the folded light curve) are highly important, along with statistical features related to flux variations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps & Advanced Techniques\n",
    "\n",
    "This notebook provides a foundational example. Here are crucial next steps to build a more robust and realistic exoplanet detection system:\n",
    "\n",
    "1.  **Real Data Acquisition:** Download actual light curves from the [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/docs/data.html) or directly using `lightkurve` from missions like Kepler or TESS. This will involve handling real-world noise, data gaps, and instrument systematics.\n",
    2.  **Periodogram Analysis:** For each stellar light curve, use `lightkurve.periodogram.LombScarglePeriodogram` to find the most likely period(s) before folding. This is a critical step for automated detection.\n",
    3.  **Advanced Feature Engineering:** Experiment with more sophisticated features such as wavelet transforms, auto-correlation functions, or features derived from model fitting (e.g., fitting a transit model and using its parameters as features).\n",
    4.  **Deep Learning (1D CNNs):** Instead of explicit feature engineering, 1D Convolutional Neural Networks (CNNs) can be fed the raw or simply preprocessed (normalized, flattened) light curve arrays. CNNs excel at learning spatial/temporal patterns directly from data.\n",
    "\n",
    "    ```python\n",
    "    # Conceptual Code for 1D CNN\n",
    "    # import tensorflow as tf\n",
    "    # from tensorflow.keras.models import Sequential\n",
    "    # from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "    # # Assuming `light_curves_arrays` is a list of preprocessed flux arrays, all padded to same length\n",
    "    # X_cnn = np.array([lc_array for lc_array in light_curves_arrays])\n",
    "    # X_cnn = X_cnn.reshape(X_cnn.shape[0], X_cnn.shape[1], 1) # Add channel dimension\n",
    "\n",
    "    # X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # model_cnn = Sequential([\n",
    "    #     Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(X_cnn.shape[1], 1)),\n",
    "    #     MaxPooling1D(pool_size=2),\n",
    "    #     Dropout(0.2),\n",
    "    #     Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    #     MaxPooling1D(pool_size=2),\n",
    "    #     Dropout(0.2),\n",
    "    #     Flatten(),\n",
    "    #     Dense(128, activation='relu'),\n",
    "    #     Dropout(0.5),\n",
    "    #     Dense(1, activation='sigmoid')\n",
    "    # ])\n",
    "\n",
    "    # model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # history = model_cnn.fit(X_train_cnn, y_train_cnn, epochs=20, batch_size=32, \n",
    "    #                       validation_data=(X_test_cnn, y_test_cnn), \n",
    "    #                       class_weight={0: 1., 1: (len(y_train_cnn) - sum(y_train_cnn)) / sum(y_train_cnn)}) # Handle imbalance\n",
    "\n",
    "    # loss, accuracy = model_cnn.evaluate(X_test_cnn, y_test_cnn)\n",
    "    # print(f\"\\nCNN Test Accuracy: {accuracy:.4f}\")\n",
    "    ```\n",
    "\n",
    "5.  **Hyperparameter Tuning:** Use techniques like GridSearchCV or RandomizedSearchCV for traditional ML models, or Keras Tuner for deep learning models, to find optimal hyperparameters.\n",
    "6.  **Ensemble Methods:** Combine predictions from multiple models (e.g., Random Forest and a CNN) for potentially higher accuracy.\n",
    "7.  **Unsupervised/Semi-supervised Learning:** Explore methods that can leverage the vast amount of unlabeled light curve data.\n",
    "\n",
    "This project is a fantastic entry point into applying advanced data science to real astronomical challenges\! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}